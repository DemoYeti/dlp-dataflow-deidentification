name: Test INSPECT, DEID & REID

on:
  pull_request:
    types: [ opened, reopened, synchronize ]
  schedule:
    - cron: '30 9 * * *'

env:
  PROJECT_ID: "dlp-dataflow-deid-ci-392604"
  DATASET_ID: "demo_dataset"
  GCS_BUCKET: "dlp-dataflow-deid-ci-392604-demo-data"
  GCS_NOTIFICATION_TOPIC: "projects/dlp-dataflow-deid-ci-392604/topics/dlp-dataflow-deid-ci-gcs-notification-topic"
  INPUT_FILE_NAME: "tiny_csv"
  INSPECT_TEMPLATE_PATH: "projects/dlp-dataflow-deid-ci-392604/locations/global/inspectTemplates/dlp-demo-inspect-latest-1689137435622"
  DEID_TEMPLATE_PATH: "projects/dlp-dataflow-deid-ci-392604/locations/global/deidentifyTemplates/dlp-demo-deid-latest-1689137435622"
  REID_TEMPLATE_PATH: "projects/dlp-dataflow-deid-ci-392604/locations/global/deidentifyTemplates/dlp-demo-reid-latest-1689137435622"
  SERVICE_ACCOUNT_EMAIL: "demo-service-account@dlp-dataflow-deid-ci-392604.iam.gserviceaccount.com"
  PUBSUB_TOPIC_NAME: "demo-topic"
  INSPECTION_TABLE_ID: "dlp_inspection_result"
  NUM_INSPECTION_RECORDS: "60"
  REIDENTIFICATION_QUERY_FILE: "reid_query.sql"

jobs:
  generate-uuid:
    runs-on: ubuntu-latest

    timeout-minutes: 5

    outputs:
      output1: ${{ steps.gen-uuid.outputs.uuid }}
      output2: ${{ steps.gen-uuid.outputs.inpect_job_name }}
      output3: ${{ steps.gen-uuid.outputs.deid_job_name }}
      output4: ${{ steps.gen-uuid.outputs.reid_job_name }}
      output5: ${{ steps.gen-uuid.outputs.dataset_id }}

    steps:
      - name: Generate UUID for workflow
        id: gen-uuid
        run: |
          new_uuid=$(uuidgen)
          modified_uuid=$(echo "$new_uuid" | tr '-' '_')
          echo "uuid=$new_uuid" >> "$GITHUB_OUTPUT"
          echo "inpect_job_name=inspect-$new_uuid" >> "$GITHUB_OUTPUT"
          echo "deid_job_name=deid-$new_uuid" >> "$GITHUB_OUTPUT"
          echo "reid_job_name=reid-$new_uuid" >> "$GITHUB_OUTPUT"
          echo "dataset_id=${{ env.DATASET_ID }}_$modified_uuid" >> "$GITHUB_OUTPUT"

  create-dataset:
    needs:
      - generate-uuid

    runs-on:
      - self-hosted

    timeout-minutes: 5

    steps:
      - name: Create BQ dataset
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          bq --location=US mk -d --description "GitHub CI workflow dataset" ${{ env.DATASET_ID }}

  inspection:
    needs:
      - generate-uuid
      - create-dataset

    runs-on:
      - self-hosted

    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v2

      - name: Setup Java
        uses: actions/setup-java@v1
        with:
          java-version: 11

      - name: Setup Gradle
        uses: gradle/gradle-build-action@v2

      - name: Run DLP Pipeline
        env:
          INSPECT_JOB_NAME: ${{ needs.generate-uuid.outputs.output2 }}
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          gradle run -DmainClass=com.google.swarm.tokenization.DLPTextToBigQueryStreamingV2 -Pargs=" \
                --streaming --enableStreamingEngine \
                --region=us-central1 \
                --project=${{env.PROJECT_ID}} \
                --tempLocation=gs://${{env.GCS_BUCKET}}/temp \
                --numWorkers=1 \
                --maxNumWorkers=2 \
                --runner=DataflowRunner \
                --filePattern=gs://${{env.GCS_BUCKET}}/${{env.INPUT_FILE_NAME}}*.csv \
                --dataset=${{env.DATASET_ID}} \
                --workerMachineType=n1-highmem-4 \
                --inspectTemplateName=${{env.INSPECT_TEMPLATE_PATH}} \
                --batchSize=200000 \
                --DLPMethod=INSPECT \
                --serviceAccount=$SERVICE_ACCOUNT_EMAIL \
                --jobName=${{env.INSPECT_JOB_NAME}} \
                --gcsNotificationTopic=${{env.GCS_NOTIFICATION_TOPIC}}"
          sleep 30s
          gsutil cp gs://${{env.GCS_BUCKET}}/temp/csv/tiny_csv_pub_sub.csv gs://${{env.GCS_BUCKET}}

      - name: Verify BQ table
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          not_verified=true
          table_count=0
          while $not_verified; do
            table_count=$(($(bq query --use_legacy_sql=false --format csv 'SELECT * FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}`.__TABLES__ WHERE table_id="${{env.INSPECTION_TABLE_ID}}"'  | wc -l ) -1))
            if [[ "$table_count" == "1" ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
            echo "Got number of tables in BQ with id ${{env.INSPECTION_TABLE_ID}}: $table_count ."
          done
          echo "Verified number of tables in BQ with id ${{env.INSPECTION_TABLE_ID}}: $table_count ."

      - name: Verify distinct rows
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          not_verified=true
          row_count=0
          while $not_verified; do
            row_count_json=$(bq query --use_legacy_sql=false --format json 'SELECT COUNT(*) FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INSPECTION_TABLE_ID}}`')
            row_count=$(echo "$row_count_json" | jq -r '.[].f0_')
            if [[ "$row_count" == ${{env.NUM_INSPECTION_RECORDS}} ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
            echo "Got number of rows in ${{env.INSPECTION_TABLE_ID}}: $row_count."
          done
          echo "Verified number of rows in ${{env.INSPECTION_TABLE_ID}}: $row_count."

  de-identification:
    needs:
      - generate-uuid
      - create-dataset

    runs-on:
      - self-hosted

    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v2

      - name: Setup Java
        uses: actions/setup-java@v1
        with:
          java-version: 11

      - name: Setup Gradle
        uses: gradle/gradle-build-action@v2

      - name: Run DLP Pipeline
        env:
          DEID_JOB_NAME: ${{ needs.generate-uuid.outputs.output3 }}
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          gradle run -DmainClass=com.google.swarm.tokenization.DLPTextToBigQueryStreamingV2 -Pargs=" \
                --streaming --enableStreamingEngine \
                --region=us-central1 \
                --project=${{env.PROJECT_ID}} \
                --tempLocation=gs://${{env.GCS_BUCKET}}/temp \
                --numWorkers=2 \
                --maxNumWorkers=3 \
                --runner=DataflowRunner \
                --filePattern=gs://${{env.GCS_BUCKET}}/${{env.INPUT_FILE_NAME}}*.csv \
                --dataset=${{env.DATASET_ID}} \
                --workerMachineType=n1-highmem-4 \
                --inspectTemplateName=${{env.INSPECT_TEMPLATE_PATH}} \
                --deidentifyTemplateName=${{env.DEID_TEMPLATE_PATH}} \
                --batchSize=200000 \
                --DLPMethod=DEID \
                --serviceAccount=${SERVICE_ACCOUNT_EMAIL} \
                --jobName=${{env.DEID_JOB_NAME}} \
                --gcsNotificationTopic=${{env.GCS_NOTIFICATION_TOPIC}}"
          sleep 30s
          if gsutil stat gs://$BUCKET_NAME/$FILE_NAME; then
            echo "Pub Sub CSV File exists hence need not copy anymore"
          else
            gsutil cp gs://${{env.GCS_BUCKET}}/temp/csv/tiny_csv_pub_sub.csv gs://${{env.GCS_BUCKET}}
          fi

      - name: Verify BQ tables
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          not_verified=true
          table_count=0
          while $not_verified; do
            table_count=$(($(bq query --use_legacy_sql=false --format csv 'SELECT * FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}`.__TABLES__ WHERE table_id="${{env.INPUT_FILE_NAME}}*"' | wc -l ) -1))
            if [[ "$table_count" == "2" ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
            echo "Got number of tables in BQ with id ${{env.INPUT_FILE_NAME}}*: $table_count ."
          done
          echo "Verified number of tables in BQ with id ${{env.INPUT_FILE_NAME}}*: $table_count ."

      - name: Verify distinct rows
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          rc_orig=$(($(gcloud storage cat gs://${{env.GCS_BUCKET}}/${{env.INPUT_FILE_NAME}}.csv | wc -l ) -1))
          not_verified=true
          row_count=0
          while $not_verified; do
            row_count_json=$(bq query --use_legacy_sql=false --format json 'SELECT COUNT(DISTINCT(ID)) FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}`')
            row_count=$(echo "$row_count_json" | jq -r '.[].f0_')
            if [[ "$row_count" == "$rc_orig" ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
            echo "Got number of rows in ${{env.INPUT_FILE_NAME}}: $row_count."
          done
          rc_orig=$(($(gcloud storage cat gs://${{env.GCS_BUCKET}}/${{env.INPUT_FILE_NAME}}_pub_sub.csv | wc -l ) -1))
          not_verified=true
          row_count=0
          while $not_verified; do
            row_count_json=$(bq query --use_legacy_sql=false --format json 'SELECT COUNT(DISTINCT(ID)) FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}_pub_sub`')
            row_count=$(echo "$row_count_json" | jq -r '.[].f0_')
            if [[ "$row_count" == "$rc_orig" ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
            echo "Got number of rows in ${{env.INPUT_FILE_NAME}}_pub_sub: $row_count."
          done
          echo "# records in input CSV file are: $rc_orig."
          echo "Verified number of rows in ${{env.INPUT_FILE_NAME}}: $row_count."

  re-identification:
    needs:
      - generate-uuid
      - create-dataset
      - de-identification

    runs-on:
      - self-hosted

    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v2

      - name: Setup Java
        uses: actions/setup-java@v1
        with:
          java-version: 11

      - name: Setup Gradle
        uses: gradle/gradle-build-action@v2

      - name: Store query in GCS bucket
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          export QUERY="SELECT ID, Card_Number, Card_Holders_Name FROM \`${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}\`"
          cat << EOF | gsutil cp - gs://${GCS_BUCKET}/${{env.REIDENTIFICATION_QUERY_FILE}}
          ${QUERY}
          EOF

      - name: Create a PubSub topic
        run: |
          if [[ $(gcloud pubsub topics list --filter="name:${{env.PUBSUB_TOPIC_NAME}}") ]]; then
           echo "Topic already created!"
          else
             gcloud pubsub topics create ${{env.PUBSUB_TOPIC_NAME}}
             echo "Created a new topic!"
          fi

      - name: Run DLP Pipeline
        env:
          REID_JOB_NAME: ${{ needs.generate-uuid.outputs.output4 }}
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          gradle run -DmainClass=com.google.swarm.tokenization.DLPTextToBigQueryStreamingV2 -Pargs=" \
                --region=us-central1 \
                --project=${{env.PROJECT_ID}} \
                --tempLocation=gs://${{env.GCS_BUCKET}}/temp \
                --numWorkers=1 \
                --maxNumWorkers=2 \
                --runner=DataflowRunner \
                --tableRef=${{env.PROJECT_ID}}:${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}} \
                --dataset=${{env.DATASET_ID}} \
                --topic=projects/${{env.PROJECT_ID}}/topics/${{env.PUBSUB_TOPIC_NAME}} \
                --autoscalingAlgorithm=THROUGHPUT_BASED \
                --workerMachineType=n1-highmem-4 \
                --deidentifyTemplateName=${{env.REID_TEMPLATE_PATH}} \
                --DLPMethod=REID \
                --keyRange=1024 \
                --queryPath=gs://${GCS_BUCKET}/${{env.REIDENTIFICATION_QUERY_FILE}} \
                --serviceAccount=${SERVICE_ACCOUNT_EMAIL} \
                --jobName=${{env.REID_JOB_NAME}}"

      - name: Verify BQ table
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          not_verified=true
          table_count=0
          while $not_verified; do
            table_count=$(($(bq query --use_legacy_sql=false --format csv 'SELECT * FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}`.__TABLES__ WHERE table_id="${{env.INPUT_FILE_NAME}}_re_id"'  | wc -l ) -1))
            if [[ "$table_count" == "1" ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
          done
          echo "Verified number of tables in BQ with id ${{env.INPUT_FILE_NAME}}_re_id: $table_count ."

      - name: Verify distinct rows
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          rc_orig=$(($(bq query --nouse_legacy_sql --format=csv "$(gcloud storage cat gs://${{env.GCS_BUCKET}}/${{env.REIDENTIFICATION_QUERY_FILE}})" | wc -l) - 1))
          not_verified=true
          row_count=0
          while $not_verified; do
            row_count_json=$(bq query --use_legacy_sql=false --format json 'SELECT COUNT(ID) FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}_re_id`')
            row_count=$(echo "$row_count_json" | jq -r '.[].f0_')
            if [[ "$row_count" == "$rc_orig" ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
          done
          echo "# records in input query are: $rc_orig."
          echo "Verified number of rows in ${{env.INPUT_FILE_NAME}}_re_id: $row_count."

  clean-up:
    if: "!cancelled()"
    needs:
      - generate-uuid
      - inspection
      - de-identification
      - re-identification

    runs-on:
      - self-hosted

    timeout-minutes: 30

    steps:
      - name: Clean-up BQ dataset and GCS Bucket
        if: "!cancelled()"
        env:
          DATASET_ID: ${{ needs.generate-uuid.outputs.output5 }}
        run: |
          bq rm -r -f -d ${{env.PROJECT_ID}}:${{env.DATASET_ID}}
          gsutil rm -f gs://${{env.GCS_BUCKET}}/${{env.INPUT_FILE_NAME}}_pub_sub.csv

      - name: Cancel Inspection Job
        if: "!cancelled()"
        env:
          INSPECT_JOB_NAME: ${{ needs.generate-uuid.outputs.output2 }}
        run: |
          inspect_job_data=$(gcloud dataflow jobs list --project ${{env.PROJECT_ID}} --status active --format json --filter="name=${{env.INSPECT_JOB_NAME}}")
          inspect_job_id=$(echo "$inspect_job_data" | jq -r '.[].id')
          if [[ "$inspect_job_id" == "" ]]; then
            echo "No job found with name: ${{env.INSPECT_JOB_NAME}}."
          else
            gcloud dataflow jobs cancel $inspect_job_id --project ${{env.PROJECT_ID}}
          fi

      - name: Cancel De-identification Job
        if: "!cancelled()"
        env:
          DEID_JOB_NAME: ${{ needs.generate-uuid.outputs.output3 }}
        run: |
          deid_job_data=$(gcloud dataflow jobs list --project ${{env.PROJECT_ID}} --status active --format json --filter="name=${{env.DEID_JOB_NAME}}")
          deid_job_id=$(echo "$deid_job_data" | jq -r '.[].id')
          if [[ "$deid_job_id" == "" ]]; then
            echo "No job found with name: ${{env.DEID_JOB_NAME}}."
          else
            gcloud dataflow jobs cancel $deid_job_id --project ${{env.PROJECT_ID}}
          fi
